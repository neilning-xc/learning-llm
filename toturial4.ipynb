{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f10199-55a6-44a7-aa3a-b94ed2857fb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 使用无标记数据预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739154af-49a0-487c-8fd2-e2a1e31cf5e7",
   "metadata": {},
   "source": [
    "没有进行训练之前的输出效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0de08d-4601-4bc0-bf2a-2f80a8cbfe86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from gpt import generate_text_simple, GPTModel\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\n",
    "\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dd961-dacd-4f19-9efa-8432aa90480c",
   "metadata": {},
   "source": [
    "## 使用交叉熵计算文本损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d1a44-0cef-449c-906a-40b3f3752594",
   "metadata": {},
   "source": [
    "首先需要了解模型训练的本质，假设有以下两个输入和期望的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87c7059-9dd0-44e5-9458-01a24b6ff556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # 将词库中的每个token都转化为一个概率值\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True) \n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d27e8a-ca26-405c-b653-36bb6f371a85",
   "metadata": {},
   "source": [
    "argmax函数返回50257维向量中，概率值最大的那个位置所对应的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fad41-1fe5-4ef4-b106-db87c86a8cf1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/proba-index.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8baa970-5522-4b73-abf9-10213e0834ca",
   "metadata": {},
   "source": [
    "**训练使得目标值对应的那个位置上的概率值最大**，接近于1，那如何用数学的方法来计算预测值和目标的的距离呢？可以使用PyTorch中的`cross_entropy`函数（交叉熵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a5be0f-a676-4a23-bc86-1f7d58dd1ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n",
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n",
    "perplexity = torch.exp(loss) # 计算交叉熵的指数\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12793f4-2ef6-40d7-ba8a-dde6e1e6ddc8",
   "metadata": {},
   "source": [
    "> 最大化目标 token 的 softmax 概率值？整体思路是通过更新模型权重，使模型在生成目标 token 时输出更高的概率值。权重更新通过一种称为反向传播的过程来实现，这是一种训练深度神经网络的标准技术\n",
    "> 反向传播需要一个损失函数，该函数用于计算模型预测输出与实际目标输出之间的差异（此处指与目标 token ID 对应的概率）。这个损失函数用于衡量模型预测与目标值的偏差程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ade42-990c-424e-bb15-5ac3258c6451",
   "metadata": {},
   "source": [
    "### 计算训练集和验证集损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daea8d61-fb44-4b0f-8711-2b1d502e41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:    \n",
    "    text_data = file.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "from gpt import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "# 将文章切分为两部分\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b765d1-281c-4b99-9ccd-02d96a40c8f4",
   "metadata": {},
   "source": [
    "定义两个函数用于计算交叉熵损失，以便评估训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcfb1632-7073-43dd-9cfc-c31fa87a4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten()) # 计算交叉熵损失\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c1bd83-99c9-44e1-9748-00db92398570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n",
      "Training loss: 10.987582630581326\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0894aa-041c-45b0-81ee-fefb3230a668",
   "metadata": {},
   "source": [
    "### 训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad8919-3daa-4bd4-b255-fb3918cc8401",
   "metadata": {},
   "source": [
    "有了上面两个评估损失的函数之后，就可以进行训练了，为了简单起见，这里创建一个简单的训练函数（还有其他更复杂的训练技术）,函数的流程图如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf73fa-000b-4538-a033-47bb530e7c8d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/train-steps.webp\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07be8cc8-3db6-452b-9032-e405dcb7ab59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "# 评价模型的训练效果\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# 打印结果\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312351d-b7c0-48a3-a536-15f924b097ed",
   "metadata": {},
   "source": [
    "调用训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bad4f581-afc2-4650-b78f-efef374dee22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.817, Val loss 9.924\n",
      "Ep 1 (Step 000005): Train loss 8.066, Val loss 8.332\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.619, Val loss 7.042\n",
      "Ep 2 (Step 000015): Train loss 6.046, Val loss 6.596\n",
      "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,, and,, the,, the, and,, and,,, the, and,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 5.524, Val loss 6.508\n",
      "Ep 3 (Step 000025): Train loss 5.369, Val loss 6.378\n",
      "Every effort moves you, and to the of the of the picture. Gis.                                     \n",
      "Ep 4 (Step 000030): Train loss 4.830, Val loss 6.263\n",
      "Ep 4 (Step 000035): Train loss 4.586, Val loss 6.285\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the picture\"I had the picture\"I the picture and I had been the picture of\n",
      "Ep 5 (Step 000040): Train loss 3.879, Val loss 6.130\n",
      "Every effort moves you know he had been his pictures, and I felt it's by his last word.                   \"Oh, and he had been the end, and he had been\n",
      "Ep 6 (Step 000045): Train loss 3.530, Val loss 6.183\n",
      "Ep 6 (Step 000050): Train loss 2.960, Val loss 6.123\n",
      "Every effort moves you know it was his pictures--I glanced after him, I had the last word.        \"Oh, and I was his pictures--I looked.   \"I looked. \"I looked. \n",
      "Ep 7 (Step 000055): Train loss 2.832, Val loss 6.150\n",
      "Ep 7 (Step 000060): Train loss 2.104, Val loss 6.133\n",
      "Every effort moves you know the picture to me--I glanced after him, and Mrs.  \"I was no great, the fact, the fact that, the moment--as Jack himself, as his pictures--as of the picture--because he was a little\n",
      "Ep 8 (Step 000065): Train loss 1.691, Val loss 6.186\n",
      "Ep 8 (Step 000070): Train loss 1.391, Val loss 6.230\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a little: \"Yes--and by me to me to have to see a smile behind his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.059, Val loss 6.251\n",
      "Ep 9 (Step 000080): Train loss 0.800, Val loss 6.278\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the fact with a laugh: \"Yes--and by me!\"  He laughed again, and threw back the window-curtains, I saw that, and down the room, and now\n",
      "Ep 10 (Step 000085): Train loss 0.569, Val loss 6.373\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Training completed in 1.11 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee715626-9100-41bb-a0fe-3055ed09ae55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "查看训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "199d7441-afdc-4ba0-8307-7bf7de9cab80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26ec04-1cdc-46eb-b127-060e5e171328",
   "metadata": {},
   "source": [
    "可以看到由于训练的数据集太小，且训练重复多次，训练结果出现了过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc93a4f-89bd-4721-aa88-a120b078f7b5",
   "metadata": {},
   "source": [
    "### 控制模型输出的随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd7318-77d1-4fe8-8db0-af1fadff0dc3",
   "metadata": {},
   "source": [
    "调整之前，多次运行`generate_text_simple`，输出都是一致的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f772b2bb-8fb1-4aa9-88a5-79ccaa6ef5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeda98f-64bd-47de-b4c7-af2f34ee1b9a",
   "metadata": {},
   "source": [
    "多次运行以上函数，每次的输出都是一致的。可以使用`temperature scaling`和`top-k sampling`来控制模型的随机性，使得模型的输出更多样性，修改`generate_text_simple`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf98f3ef-5e0a-487f-bc38-350e33dab248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d198be88-9fe8-4ee6-9def-96c8c18c74c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you stand,\" she down.\" For Mrs. Gisitely told Mrs. St\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8818b2b-5eb0-4ce7-89fe-473cbf761e21",
   "metadata": {},
   "source": [
    "### 加载和保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdf9f1de-a07f-40a5-b21a-afe46f833aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"model_and_optimizer.pth\"\n",
    ")\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", weights_only=True)\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe7b97-f361-48e7-8df1-5e1b55016202",
   "metadata": {},
   "source": [
    "### 从OpenAI加载已经训练好的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81924eb4-03fe-43d8-ac82-ff6a7fa5b19b",
   "metadata": {},
   "source": [
    "自己从零开始训练的成本比较高，所以我们可以直接下载并加载别人已经训练好的开源模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86d9deca-ed84-47c7-8352-1ff7a8d47c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Output text:\n",
      " Every effort moves you toward finding an ideal life. You don't have to accept your problems by trying to remedy them, because that would be foolish\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# import requests\n",
    "import tensorflow as tf\n",
    "import tiktoken\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import from local files\n",
    "# from previous_chapters import GPTModel\n",
    "\n",
    "\n",
    "# def text_to_token_ids(text, tokenizer):\n",
    "#     encoded = tokenizer.encode(text)\n",
    "#     encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "#     return encoded_tensor\n",
    "\n",
    "\n",
    "# def token_ids_to_text(token_ids, tokenizer):\n",
    "#     flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "#     return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination):\n",
    "    # Send a GET request to download the file\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        # Get the total file size from headers, defaulting to 0 if not present\n",
    "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "        # Check if file exists and has the same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return\n",
    "\n",
    "        # Define the block size for reading the file\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        # Initialize the progress bar with total file size\n",
    "        progress_bar_description = os.path.basename(url)  # Extract filename from URL\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(destination, \"wb\") as file:\n",
    "                # Read the file in chunks and write to destination\n",
    "                while True:\n",
    "                    chunk = response.read(block_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    file.write(chunk)\n",
    "                    progress_bar.update(len(chunk))  # Update progress bar\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params[\"wpe\"])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params[\"wte\"])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "# def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "#     # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "#     for _ in range(max_new_tokens):\n",
    "#         idx_cond = idx[:, -context_size:]\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(idx_cond)\n",
    "#         logits = logits[:, -1, :]\n",
    "\n",
    "#         # New: Filter logits with top_k sampling\n",
    "#         if top_k is not None:\n",
    "#             # Keep only top_k values\n",
    "#             top_logits, _ = torch.topk(logits, top_k)\n",
    "#             min_val = top_logits[:, -1]\n",
    "#             logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "#         # New: Apply temperature scaling\n",
    "#         if temperature > 0.0:\n",
    "#             logits = logits / temperature\n",
    "\n",
    "#             # Apply softmax to get probabilities\n",
    "#             probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "#             # Sample from the distribution\n",
    "#             idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "#         # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "#         else:\n",
    "#             idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "#         if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "#             break\n",
    "\n",
    "#         # Same as before: append sampled index to the running sequence\n",
    "#         idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "#     return idx\n",
    "\n",
    "\n",
    "def main(gpt_config, input_prompt, model_size):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "    gpt = GPTModel(gpt_config)\n",
    "    load_weights_into_gpt(gpt, params)\n",
    "    gpt.to(device)\n",
    "    gpt.eval()\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=gpt,\n",
    "        idx=text_to_token_ids(input_prompt, tokenizer).to(device),\n",
    "        max_new_tokens=25,\n",
    "        context_size=gpt_config[\"context_length\"],\n",
    "        top_k=50,\n",
    "        temperature=1.0\n",
    "    )\n",
    "\n",
    "    print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "    INPUT_PROMPT = \"Every effort moves you\"\n",
    "\n",
    "    BASE_CONFIG = {\n",
    "        \"vocab_size\": 50257,     # Vocabulary size\n",
    "        \"context_length\": 1024,  # Context length\n",
    "        \"drop_rate\": 0.0,        # Dropout rate\n",
    "        \"qkv_bias\": True         # Query-key-value bias\n",
    "    }\n",
    "\n",
    "    model_configs = {\n",
    "        \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "        \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "\n",
    "    model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "    BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "    main(BASE_CONFIG, INPUT_PROMPT, model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b3482-2a21-41fd-b894-94103ebc9642",
   "metadata": {},
   "source": [
    "可以看到输出的语法不仅正确了，文本的上下文含义也更合理了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294759c-019b-4991-bb27-a3a922c1f49b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
