{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f003ad-5b02-4054-82e3-089e50b68f14",
   "metadata": {},
   "source": [
    "# 从零实现一个大模型（二）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a46550-2156-4326-8d66-1864ab5bfa8c",
   "metadata": {},
   "source": [
    "本文从零实现一个大模型，用于帮助理解大模型的基本原理，本文是一个读书笔记，内容来自于[Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefef77-7ffb-4ec2-b7b3-43581da888a1",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb53bd-58e9-4397-9d2e-dc5b69ae6b62",
   "metadata": {},
   "source": [
    "本系列包含以以下主题，当前在主题二"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e6acf-0f2d-47bd-a59f-7c58e96370d1",
   "metadata": {},
   "source": [
    "### 1. 文本处理\n",
    "### 2. 注意力机制\n",
    "### 3. 开发一个Transform架构的大模型\n",
    "### 4. 使用无标记数据预训练模型\n",
    "### 5. 分类微调\n",
    "### 6. 指令微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a86d9d8-2f3e-41da-b18c-7dee6162a60d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a0ce4-4721-4966-97d4-9b7a2fa0c98c",
   "metadata": {},
   "source": [
    "注意力机制是Transformer架构中一种技术，目的是能够将一个长序列中每个位置上元素都能和其他位置上的元素产生关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8da1fc-1001-4b7b-bf41-926ee9302a44",
   "metadata": {},
   "source": [
    "## 不含可训练权重的简单自注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd8705-5ef3-46a6-ac44-6d5075300298",
   "metadata": {},
   "outputs": [],
   "source": [
    "为了充分理解自注意力机制的原理，我们从简单的自注意力机制开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684ed75-04f4-4bb4-89cf-845c58409fe6",
   "metadata": {},
   "source": [
    "要获得上下文向量，需要使用使用输入向量乘以注意力权重，大致过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004581a1-abb1-4d71-820c-c30afb5f45e3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484baa95-8bd7-43ba-b56c-f166b6dc4e1f",
   "metadata": {},
   "source": [
    "### 计算注意力评分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b094616-0308-44f5-b999-2a5154ade1f8",
   "metadata": {},
   "source": [
    "想要得到注意力权重，首先**第一步要获取注意力评分**，单个输入的注意力评分的计算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e04444-b703-4e12-bee0-98ced8ff5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1724945e-d212-46e0-beba-3c32e260198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 以第二个输入为例\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # 计算第二个元素的的注意力分数\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # 点积运算\n",
    "\n",
    "print(attn_scores_2) # 打印第二个输入词的注意力分数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea5184-42d8-4a30-b469-1f5d58c67f4f",
   "metadata": {},
   "source": [
    "以上计算的本质就是其他六个单词相对于第二个单词“journey”注意力分数，所有他的长度是6，即inputs的总长度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662ba59-d7ff-4197-ae64-9c81ba8deabb",
   "metadata": {},
   "source": [
    "### 计算注意力权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8052b5-4fa3-4ec1-ac84-541cace2f6d8",
   "metadata": {},
   "source": [
    "通过上一步计算得到注意力分数之后，**第二部需要计算得到注意力权重**只需要将注意力分数进行序列化（或者叫归一化），即可得到注意力权重，注意力权重的特点是总和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef99e183-8782-4999-b15c-bca1257c107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdb683-df42-4c52-ac48-64690aff7ae7",
   "metadata": {},
   "source": [
    "以上演示了的到注意力权重的大致过程，现实中我们可以直接`torch`库提供的现成函数`softmax`计算注意力权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5d5539-21e0-47c7-8741-1273d35dd0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd26ed8-7b27-4ae0-9654-14c8eb01ec8f",
   "metadata": {},
   "source": [
    "### 计算上下文向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85f972-a424-41fd-a0d8-13bbacf06929",
   "metadata": {},
   "source": [
    "**第三步计算上下文向量**，将上面的到的注意力权重和某个token ID的嵌入向量相乘即可得到改token ID的上下文向量，注意经过计算之后，得到的这个上下文向量的维度和input的token ID向量的维度是相同的，即“journey”的上下文向量和“journey”的嵌入向量的维度是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe6c936-8203-4661-9ff6-c930344f00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 以第二个输入为例\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i # 使用上文的注意力权重和每一个input token进行乘积运算\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61fddb-2ae8-4584-81fd-a3ff1bada058",
   "metadata": {},
   "source": [
    "以上过程只是计算了单个输入的上下文向量，LLM中需要为所有的输入都计算上下文向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16fb04-186e-426f-97e0-a1a184dc126e",
   "metadata": {},
   "source": [
    "### 计算所有输入的上下文向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73f31f-c311-4fd1-b797-df8897c20965",
   "metadata": {},
   "source": [
    "了解了单个上下文向量的计算过程之后，我们可以为所有的输入计算上下文向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb7ac8e-3a65-41dc-a2c6-18934b13b788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# 1.计算注意力分数\n",
    "attn_scores = inputs @ inputs.T\n",
    "# 2.计算注意力权重\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "# 3.计算上下文向量\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0caf92-bb96-437a-bb96-5fbdb4bbc157",
   "metadata": {},
   "source": [
    "### 实现一个包含可训练权重的自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f2995-349e-4242-86c7-3e08538bf557",
   "metadata": {},
   "source": [
    "可训练权重的自注意力机制的过程和上面的过程类似，也需要为特定的输入token计算上下文向量，区别是它引入了一个新的权重指标，他会在模型训练的过程中更新。这种可训练的权重指标对LLM来说至关重要，他可以使模型通过不断的学习生成更好的上下文向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f105c9-b75f-47ed-bb5d-7966c0af3554",
   "metadata": {},
   "source": [
    "权重指标分别是$W_q$，$W_k$和$W_v$，将这三个权重指标和某个input相乘，可以得到Query、Key、Input向量："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0adfa-5b2c-456e-bc65-d6c87e0826dc",
   "metadata": {},
   "source": [
    "- Query vector: $q^{(i)} = x^{(i)}\\\\,W_q $\n",
    "- Key vector: $k^{(i)} = x^{(i)}\\\\,W_k $\n",
    "- Value vector: $v^{(i)} = x^{(i)}\\\\,W_v $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5112a0b0-81f7-4b27-af90-e750458af507",
   "metadata": {},
   "source": [
    "接下来仍然使用单词“journey”作为示例演示这个计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7bbdc9-e0f7-470d-840a-eadb4bc5238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # 第二个单词“journey”\n",
    "d_in = inputs.shape[1] # 输入的嵌入维度，3\n",
    "d_out = 2 # 输出的嵌入维度, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98af075-386a-429a-9b96-665a3202ceda",
   "metadata": {},
   "source": [
    "计算$W_q$，$W_k$和$W_v$指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c43ea9f-0f45-4e0d-a582-af8c188d4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ed63b-0ce4-4c01-8035-643126bfa43f",
   "metadata": {},
   "source": [
    "计算Query、Key、Value向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ffce63-c495-4897-ab38-d1295aa1afdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531a68b-134b-41f1-895e-96ea6b52c13c",
   "metadata": {},
   "source": [
    "上面的示例演示了单个输入的Q、K、V向量，为所有输入计算也很简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166f732d-3c33-4759-a309-9dddc4fc9355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505bafe9-0af0-4b02-a189-6b055c90834b",
   "metadata": {},
   "source": [
    "得到QKV向量之后，就可以计算注意力分数了，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67ad606-e931-4c18-a195-d83c4b2c4502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # 计算第二个单词的注意力分数\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b9a0327-e64e-451a-9068-5525e75e306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # 为所有的输入计算注意力分数\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0ca5a6-9618-4cb3-a65d-4a42350b9dff",
   "metadata": {},
   "source": [
    "第三步和上面一样，使用softmax函数将注意力分数序列化，得到注意力权重，注意力权重的总和是1。和前面的区别是进行缩放，将值除以输入维度的平方根。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eadb0f6b-99e9-4234-b197-528c56d698a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1] #input的维度\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49f2d0-7a5e-4a6c-8c8d-69d30a16f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "最后一步，计算上下文向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b02f72-b55f-404f-aedd-a2eeb78392d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be35f223-ce3c-48a1-ad6d-2c58805bba0f",
   "metadata": {},
   "source": [
    "### 实现自注意力类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f209e2-554f-47cc-afc5-d2151ebc5ffc",
   "metadata": {},
   "source": [
    "将以上代码整理后，即可实现一个自注意力的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52af6074-43dc-4ff9-87cf-c5c5bb546d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40fec7-d8a1-4a31-9d9c-291d3e22f20c",
   "metadata": {},
   "source": [
    "上面的代码实现中使用nn.Linear替换了nn.Parameter(torch.rand(...)，因为该方法有更好的权重初始化原型，会让模型的训练更稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ede4d0-0648-47ed-9b59-405ab0dd0c3d",
   "metadata": {},
   "source": [
    "### 实现因果自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c1531-97d8-4441-ba15-186ed71d92e0",
   "metadata": {},
   "source": [
    "在因果自注意力机制中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c7948-9a2a-485a-a468-d3fdcb7caf6e",
   "metadata": {},
   "source": [
    "### 使用上下文向量实现因果自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2941a-1762-4282-a25f-2af5c6ca876c",
   "metadata": {},
   "source": [
    "- 因果自注意力机制确保某个输入的上下文向量值只和前面的输入相关\n",
    "- 这确保了下一个单词的预测只依赖于前边的单词\n",
    "- 为了实现因果自注意力机制，我们需要使用一个mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4b8cc-c310-47c3-9865-930569da178c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab43f9e-009f-4cb5-ad5f-c15397be2316",
   "metadata": {},
   "source": [
    "在实现因果注意力机制时，还需要随机丢弃一些值，来防止模型过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f994f51-b5e0-4b4a-b3b8-19203a6ba39e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9aace1-0b1f-46dc-82a1-1da17e4fe0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method. \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417958ac-387e-4b0b-9b19-60a5063954bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021c70c-adcf-41d0-9dcf-a26bd8fcc5e4",
   "metadata": {},
   "source": [
    "### 将单头注意力转化为多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69a107-a8ad-410f-abfe-eb1399db38e6",
   "metadata": {},
   "source": [
    "多头注意力和核心思想是将单个注意力重复多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f06467-413b-43ea-817c-d1a969fddb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forwar\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6643303a-a6e3-4764-ba17-21235db04c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dddd74-8670-4d57-8a05-de71eea70c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
