{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a86d9d8-2f3e-41da-b18c-7dee6162a60d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a0ce4-4721-4966-97d4-9b7a2fa0c98c",
   "metadata": {},
   "source": [
    "注意力机制是Transformer架构中一种技术，目的是能够将一个长序列中每个位置上元素都能于其他位置上的元素产生关联"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684ed75-04f4-4bb4-89cf-845c58409fe6",
   "metadata": {},
   "source": [
    "计算输入的上下文向量使用输入向量乘以注意力权重，大致过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004581a1-abb1-4d71-820c-c30afb5f45e3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484baa95-8bd7-43ba-b56c-f166b6dc4e1f",
   "metadata": {},
   "source": [
    "### 计算注意力评分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b094616-0308-44f5-b999-2a5154ade1f8",
   "metadata": {},
   "source": [
    "想要得到注意力权重，首先要获取注意力评分，注意力评分的计算过程如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e04444-b703-4e12-bee0-98ced8ff5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1724945e-d212-46e0-beba-3c32e260198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # 以第二个输入为例\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # 第二个输入的注意力分数\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # 点积运算\n",
    "\n",
    "print(attn_scores_2) # 打印第二个输入词的注意力分数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662ba59-d7ff-4197-ae64-9c81ba8deabb",
   "metadata": {},
   "source": [
    "### 计算注意力权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8052b5-4fa3-4ec1-ac84-541cace2f6d8",
   "metadata": {},
   "source": [
    "的到注意力分数之后，只需要将注意力分数进行序列化（或者叫归一化），即可得到注意力权重，注意力权重的总和为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef99e183-8782-4999-b15c-bca1257c107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdb683-df42-4c52-ac48-64690aff7ae7",
   "metadata": {},
   "source": [
    "以上演示了的到注意力权重的大致过程，现实中我们可以直接`torch`库提供的现成函数softmax计算注意力权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5d5539-21e0-47c7-8741-1273d35dd0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd26ed8-7b27-4ae0-9654-14c8eb01ec8f",
   "metadata": {},
   "source": [
    "### 计算上下文向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85f972-a424-41fd-a0d8-13bbacf06929",
   "metadata": {},
   "source": [
    "下一步使用上面的到的注意力权重计算的到某个特定输入的上下文向量，这个上下文向量的维度和input向量的维度是相同的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe6c936-8203-4661-9ff6-c930344f00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 以第二个输入为例\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i # 使用上文的注意力权重和每一个input token进行乘积运算\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b61fddb-2ae8-4584-81fd-a3ff1bada058",
   "metadata": {},
   "source": [
    "以上过程只是计算了单个输入的上下文向量，LLM中需要为所有的输入都计算上下文向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c7948-9a2a-485a-a468-d3fdcb7caf6e",
   "metadata": {},
   "source": [
    "### 使用上下文向量实现因果自注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2941a-1762-4282-a25f-2af5c6ca876c",
   "metadata": {},
   "source": [
    "- 因果自注意力机制确保某个输入的上下文向量值只和前面的输入相关\n",
    "- 这确保了下一个单词的预测只依赖于前边的单词\n",
    "- 为了实现因果自注意力机制，我们需要使用一个mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4b8cc-c310-47c3-9865-930569da178c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab43f9e-009f-4cb5-ad5f-c15397be2316",
   "metadata": {},
   "source": [
    "在实现因果注意力机制时，还需要随机丢弃一些值，来防止模型过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f994f51-b5e0-4b4a-b3b8-19203a6ba39e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9aace1-0b1f-46dc-82a1-1da17e4fe0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        # For inputs where `num_tokens` exceeds `context_length`, this will result in errors\n",
    "        # in the mask creation further below.\n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method. \n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417958ac-387e-4b0b-9b19-60a5063954bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021c70c-adcf-41d0-9dcf-a26bd8fcc5e4",
   "metadata": {},
   "source": [
    "### 将单头注意力转化为多头注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69a107-a8ad-410f-abfe-eb1399db38e6",
   "metadata": {},
   "source": [
    "多头注意力和核心思想是将单个注意力重复多次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88f06467-413b-43ea-817c-d1a969fddb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forwar\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6643303a-a6e3-4764-ba17-21235db04c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dddd74-8670-4d57-8a05-de71eea70c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
