{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad96bd03-4184-493f-aca1-0e44c23c60cc",
   "metadata": {},
   "source": [
    "# 从零实现一个大模型（三）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4059150-eefe-4673-8fe7-741b9d4cfa9f",
   "metadata": {},
   "source": [
    "本文从零实现一个大模型，用于帮助理解大模型的基本原理，本文是一个读书笔记，内容来自于[Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf8106-9fb5-424f-868f-853ae60a10ae",
   "metadata": {},
   "source": [
    "## 目录"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cff871-111b-4f35-b277-b3285e020de4",
   "metadata": {},
   "source": [
    "本系列包含以以下主题，当前在主题三"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b22053-0088-450f-b949-644104ae0e42",
   "metadata": {},
   "source": [
    "### 1. 文本处理\n",
    "### 2. 注意力机制\n",
    "### 3. 开发一个Transform架构的大模型\n",
    "### 4. 使用无标记数据预训练模型\n",
    "### 5. 分类微调\n",
    "### 6. 指令微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bde2f-084e-4788-afcc-ad35594b9fdf",
   "metadata": {},
   "source": [
    "上一章内容实现了注意力机制，注意力机制的目标是生成上下文向量，这一章的会开发类GPT-2的模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f57b7f-003b-4ebd-af0f-2d5b8e37e858",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264c0b0-fdb3-47fa-bceb-759cac1659d4",
   "metadata": {},
   "source": [
    "类GPT模型的架构图如下，该类主要包含以下两个个未实现的模块：Output layers和Transformer block。其他模块在之前的章节中已经实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7dd55-5802-47c9-a83a-23316127008a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452e764-5939-46ab-9de9-bff401c96967",
   "metadata": {},
   "source": [
    "首先初始化GPTModel类所需的配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95594df3-5d0c-4d40-bfe5-8886e17ef2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # 词汇表大小\n",
    "    \"context_length\": 1024, # 上下文长度\n",
    "    \"emb_dim\": 768,         # 嵌入向量的维度\n",
    "    \"n_heads\": 12,          # 多头注意力的个数\n",
    "    \"n_layers\": 12,         # 模型中 Transformer 模块的层数\n",
    "    \"drop_rate\": 0.1,       # 丢弃率，防止模型过拟合\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias，注意力类中会用到该参数\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c040d8c-ed8f-427e-9212-70de565255b9",
   "metadata": {},
   "source": [
    "模型类的的代码结构如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e370de4e-4be3-4959-9e3a-5e02479d8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 待实现\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 待实现\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # 待实现\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 待实现\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e62eb-55a9-4f05-853c-32ddffff8819",
   "metadata": {},
   "source": [
    "### 归一化层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6bcbe-aea6-4b8c-a2b9-b2edb7ee2601",
   "metadata": {},
   "source": [
    "首先需要实现的的归一化层，即上文代码中的LayerNorm，该类的作用是将inputs的最后一个维度（嵌入向量的维度，emb_dim）进行标准化，将他们的值调整为均值为 0，方差为 1（即单位方差）。\n",
    "这样做的目的是为了防止梯度消失或者梯度爆炸，加速权重的收敛速度，确保训练过程的一致性和稳定性。归一化层用在多头注意力层之前和之后，也应用在最终的输出层之前"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ddaa60-efb0-4430-9ccf-c1d33a366446",
   "metadata": {},
   "source": [
    "> 梯度指的是变化率，描述了某个值（例如函数输出值）对另一个值（如输入变量）的变化趋势。大模型在应用梯度的概念时，首先会设计一个损失函数，用来衡量模型的预测结果与目标结果的差距。在训练过程中，它通过梯度去帮助每个模型参数不断调整来快速减少损失函数的值，从而提高模型的预测精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdaf5ed-3a6a-4fc9-b0e2-5dd48f014fc4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab5991-a0b8-4376-832f-83ac7fedc0ed",
   "metadata": {},
   "source": [
    "上图演示了一个具有5个输入和6个输出的神经网络层，输出层的6个值，其均值为0，方差为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31cb766-6f7f-4557-964a-5d5789bb8ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# 创建2个训练样本，每个样本有5个维度（特征）\n",
    "batch_example = torch.randn(2, 5) \n",
    "# 创建一个具有 5 个输入和 6 个输出的神经网络层\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "# 查看输出\n",
    "print(out)\n",
    "\n",
    "# 查看层归一化处理之前的均值和方差\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# 查看层归一化处理之前的均值和方差\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528fc6c-b72f-4f1f-8e1a-d6b9700c1eeb",
   "metadata": {},
   "source": [
    "以上是层归一化的实现原理，根据以上过程，LayerNorm的具体实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcea18bf-494e-43a7-9c9c-66c38e4e4e9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLayerNorm\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, emb_dim):\n\u001b[32m      3\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # scale 和 shift 是两个可训练参数（与输入具有相同的维度）。大语言模型（LLM）在训练中会自动调整这些参数，\n",
    "        # 以改善模型在训练任务上的性能。这使得模型能够学习适合数据处理的最佳缩放和偏移方式。\n",
    "        return self.scale * norm_x + self.shift "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a802c5-37c9-4623-908d-fa260a5b6f08",
   "metadata": {},
   "source": [
    "调用上面的层归一化类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e730f7-7077-47d4-bf2c-a81ba479f765",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LayerNorm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ln = \u001b[43mLayerNorm\u001b[49m(emb_dim=\u001b[32m5\u001b[39m)\n\u001b[32m      2\u001b[39m out_ln = ln(batch_example)\n\u001b[32m      3\u001b[39m mean = out_ln.mean(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'LayerNorm' is not defined"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac233948-fa2c-456a-aace-2844dae8e5ee",
   "metadata": {},
   "source": [
    "### 使用GELU激活函数实现前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1cc8f-daa7-4f62-8b0b-cd1a23435b2c",
   "metadata": {},
   "source": [
    "接下来需要实现一个前馈神经网络FeedForward类，但是在实现这个类之前要先了解一下GELU激活函数，GELU激活函数是更复杂、平滑的激活函数，分别结合了高斯分布和 sigmoid 门控线性单元。他可以为深度学习模型提供更好的性能。这里暂时不深究他的数学原理，直接实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965e7014-a9d5-48fc-8d86-b814310f2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85901fe-8b2f-4c3a-aa73-d004a9a35a80",
   "metadata": {},
   "source": [
    "实现了GELU激活函数之后，就可以实现前馈神经网络了，FeedForward 模块是一个小型神经网络，由两个线性层和一个 GELU 激活函数组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a688e6c-5359-4323-91b7-8594be38a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), # 线性层\n",
    "            GELU(), # 激活函数\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af8a41-5430-4c53-9a85-6824846a4972",
   "metadata": {},
   "source": [
    "前馈神经网络的输入和输出具有相同的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81135b8b-c096-48a6-bc67-19155d62cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8e846-5bf1-416e-919a-b1641c3f2885",
   "metadata": {},
   "source": [
    "这里实现的 FeedForward 模块对模型能力的增强（主要体现在从数据中学习模式并泛化方面）起到了关键作用。尽管该模块的输入和输出维度相同，但在内部，它首先通过第一个线性层将嵌入维度扩展到一个更高维度的空间。之后再接入非线性 GELU 激活，最后再通过第二个线性层变换回原始维度。这样的设计能够探索更丰富的表示空间。上面的架构如图所示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9b462-2e67-40f3-acfe-34584d1b2085",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145709e9-b089-4be4-a4dd-a8eaa09c0da8",
   "metadata": {},
   "source": [
    "### 实现快捷连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2c3466-e963-4163-9364-a9308d7a9c72",
   "metadata": {},
   "source": [
    "接下来，还要再讨论一下快捷连接（也称跳跃连接或残差连接）的概念，它用于缓解梯度消失问题。梯度消失是指在训练中指导权重更新的梯度在反向传播过程中逐渐减小，导致早期层（靠近输入端的网络层）难以有效训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05015c7-7bac-4335-a570-1d16d6b44735",
   "metadata": {},
   "source": [
    "**简单的说，快捷连接有以下两个作用：**\n",
    "1. 保持信息（或者说是特征）流畅传递\n",
    "2. 缓解梯度消失问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc379f93-3572-4de1-94d8-39f94dd0e9ea",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3f3a8a-7242-4c69-bcb5-8d5fa254a37f",
   "metadata": {},
   "source": [
    "快捷连接的实现示例如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d936a4-fc62-4b1a-9178-dcf6f905af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # 计算当前层的输出层\n",
    "            layer_output = layer(x)\n",
    "            # 如果使用了快捷连接，则将当前层和输出层直接相加\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdb508-8f82-4bb6-a557-66ec2f40222e",
   "metadata": {},
   "source": [
    "接下来实现一个函数打印梯度对比观察使用快捷连接的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35015d5-a8ff-4785-a74a-4cc05bcd295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0f8b4-ec05-454a-b03b-2f1d4de80592",
   "metadata": {},
   "source": [
    "不使用快捷连接时的结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa82d06a-29e5-49ef-bbac-3b6d8069fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a518d95-08d7-4f3e-b029-150e4379071e",
   "metadata": {},
   "source": [
    "梯度在从最后一层（layers.4）到第一层（layers.0）时逐渐减小，这种现象称为梯度消失问题。再看看使用快捷连接的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9197a608-5697-4410-bbbf-0905e20f9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be9d0c-4fba-48f8-ba1b-495877f70ce6",
   "metadata": {},
   "source": [
    "可以已经没有梯度消失的问题了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca6e13-fe12-40e2-a2ab-d93e66a28c0a",
   "metadata": {},
   "source": [
    "### 实现TransformerBlock类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df02a36d-920d-43b8-9274-fdf898ce78ff",
   "metadata": {},
   "source": [
    "以上模块实现之后，接下来就可以实现GPTModel类的核心模块TransformerBlock类了，他的结构图如下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12492c7c-babd-402f-8f11-5934f3e53295",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba458f6-4105-438d-9d14-d50099237f7c",
   "metadata": {},
   "source": [
    "根据上面的架构图，将之前实现的子模块组装成TransformerBlock类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0d55c6-cfe9-4c86-a33f-fd22a0dccda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import MultiHeadAttention # 上一章实现的多头注意力类\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # 第一次使用归一化层处理\n",
    "        x = self.att(x)  # 使用多头注意力层处理\n",
    "        x = self.drop_shortcut(x) # 随机丢弃一些值\n",
    "        x = x + shortcut  # 第一次使用快捷连接\n",
    "\n",
    "        # 快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x) # 第二次使用归一化层处理\n",
    "        x = self.ff(x) # 使用前馈网络层处理\n",
    "        x = self.drop_shortcut(x) # 随机丢弃一些值\n",
    "        x = x + shortcut  # 第二次使用快捷连接\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd86912-330c-4840-b911-99ebeaa1f955",
   "metadata": {},
   "source": [
    "### 实现GPTModel类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87c19c-3e45-4c67-b41e-2690193140a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "核心模块TransformerBlock类实现之后，就可以实现GPTModel类了，类架构图如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003bd1ab-6271-4755-8c60-4cf5f05f91af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0229f20-2ecf-4e41-8913-57c631de48f5",
   "metadata": {},
   "source": [
    "根据上面的架构图，将之前实现的函数组装成GPTModel类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a1e9ef-9c22-44a0-958d-636b3289a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # 将TransformBlock类重复12次\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx) # 获取输入的嵌入向量\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # 获取输入的位置嵌入向量\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x) # 使用TransformBlock类处理\n",
    "        x = self.final_norm(x) # 使用归一化层处理\n",
    "        logits = self.out_head(x) # 最后使用线性层处理\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed876c-4355-400b-83cc-776d673d9fbc",
   "metadata": {},
   "source": [
    "### 文本测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50023145-075a-41c0-a0ca-84f2563f3391",
   "metadata": {},
   "source": [
    "创建一个辅助函数测试上面的GPTModel类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9575c1e8-f62c-4cfa-b767-fcd27b444684",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07be8d35-20c6-4cda-ad02-c9e604ae04e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "                      IN\n",
      "==================================================\n",
      "\n",
      "Input text: Hello, I am\n",
      "Encoded input text: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n",
      "\n",
      "\n",
      "==================================================\n",
      "                      OUT\n",
      "==================================================\n",
      "\n",
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267,\n",
      "         49706, 43231, 47062, 34657]])\n",
      "Output length: 14\n",
      "Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()  # disable dropout\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = tokenizer.encode(start_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\n",
    "print(f\"\\n{50*'='}\\n{22*' '}IN\\n{50*'='}\")\n",
    "print(\"\\nInput text:\", start_context)\n",
    "print(\"Encoded input text:\", encoded)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "\n",
    "print(f\"\\n\\n{50*'='}\\n{22*' '}OUT\\n{50*'='}\")\n",
    "print(\"\\nOutput:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "print(\"Output text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ce38a-b942-4c27-9fe5-8c928357bcdb",
   "metadata": {},
   "source": [
    "可以看到，经过模型处理后，已经有输出了，但是输出的内容还是错误的，这是因为模型还没有经过任何的训练，下一章要训练模型，来输出合理的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fbd0a-44b6-47cb-93c9-705429eab117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
